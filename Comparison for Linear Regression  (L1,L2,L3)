# STUDENT PERFORMANCE PREDICTION 
# 3Advance LR - LASSO, RIDGE & ELASTICNET (with GridSearchCV)

import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.exceptions import ConvergenceWarning
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, Ridge, ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Silence common optimization warnings to keep output tidy
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# 1. Load Dataset
FILE = r"C:\Data\student_habits_performance.csv"
df = pd.read_csv(FILE)

# 2. Define Target & Features
target = "exam_score"
if target not in df.columns:
    raise ValueError(f"Target '{target}' not in columns: {list(df.columns)}")

X = df.drop(columns=[target])
y = df[target].astype(float)

# Convert non-numeric columns (like gender, etc.) to numeric
X = pd.get_dummies(X, drop_first=True)

# 3. Split Data (Train/Test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# 5. Helper Function for Evaluation & Plotting
def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    r2   = r2_score(y_test, y_pred)
    mae  = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"\n{name.upper()} REGRESSION RESULTS")
    print(f"R²   : {r2:.4f}")
    print(f"MAE  : {mae:.4f}")
    print(f"RMSE : {rmse:.4f}")

    # Plot Actual vs Predicted (parity)
    plt.figure(figsize=(6, 6))
    plt.scatter(y_test, y_pred, alpha=0.7)
    mn, mx = y_test.min(), y_test.max()
    plt.plot([mn, mx], [mn, mx], color='red', lw=2)
    plt.xlabel("Actual Exam Score")
    plt.ylabel("Predicted Exam Score")
    plt.title(f"{name} Regression: Actual vs Predicted")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Residuals plot
    residuals = y_test - y_pred
    plt.figure(figsize=(7, 5))
    plt.scatter(y_pred, residuals, alpha=0.7)
    plt.axhline(0, color='red', lw=2)
    plt.xlabel("Predicted Exam Score")
    plt.ylabel("Residual (Actual - Predicted)")
    plt.title(f"{name} Regression: Residuals")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return mae, rmse, r2

# 6–8. Models with GridSearchCV
# Scoring: use R² for selection; set cv=5 and n_jobs=-1 for speed
# Increase max_iter for stability (especially ElasticNet / Lasso)
cv = 5
n_jobs = -1

# LASSO
lasso_param_grid = {
    "alpha": [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
}
lasso_grid = GridSearchCV(
    estimator=Lasso(max_iter=10000),
    param_grid=lasso_param_grid,
    scoring="r2",
    cv=cv,
    n_jobs=n_jobs,
    refit=True,
)
lasso_grid.fit(X_train_scaled, y_train)
best_lasso = lasso_grid.best_estimator_
print("Best LASSO params:", lasso_grid.best_params_)

# RIDGE
ridge_param_grid = {
    "alpha": [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]
}
ridge_grid = GridSearchCV(
    estimator=Ridge(max_iter=10000),
    param_grid=ridge_param_grid,
    scoring="r2",
    cv=cv,
    n_jobs=n_jobs,
    refit=True,
)
ridge_grid.fit(X_train_scaled, y_train)
best_ridge = ridge_grid.best_estimator_
print("Best RIDGE params:", ridge_grid.best_params_)

# ELASTICNET
elastic_param_grid = {
    "alpha":    [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],
    "l1_ratio": [0.1, 0.3, 0.5, 0.7, 0.9]
}
elastic_grid = GridSearchCV(
    estimator=ElasticNet(max_iter=10000, random_state=42),
    param_grid=elastic_param_grid,
    scoring="r2",
    cv=cv,
    n_jobs=n_jobs,
    refit=True,
)
elastic_grid.fit(X_train_scaled, y_train)
best_elastic = elastic_grid.best_estimator_
print("Best ELASTICNET params:", elastic_grid.best_params_)

# 9. Evaluate tuned models on TEST set
lasso_metrics   = evaluate_model("Lasso (L1)", best_lasso,   X_test_scaled, y_test)
ridge_metrics   = evaluate_model("Ridge (L2)", best_ridge,   X_test_scaled, y_test)
elastic_metrics = evaluate_model("ElasticNet", best_elastic, X_test_scaled, y_test)

# 10. Coefficient comparison (using tuned models)
coef_df = pd.DataFrame({
    "Feature":     X.columns,
    "Lasso":       best_lasso.coef_,
    "Ridge":       best_ridge.coef_,
    "ElasticNet":  best_elastic.coef_
})
print("\nModel Coefficients (top 5 by |ElasticNet|):")
print(coef_df.reindex(coef_df["ElasticNet"].abs().sort_values(ascending=False).index).head(5))

# 11. Compact summary table + best model by RMSE
results = {
    "Lasso (L1)":   lasso_metrics,      # (MAE, RMSE, R2)
    "Ridge (L2)":   ridge_metrics,
    "ElasticNet":   elastic_metrics,
}

print("\nTest Metrics Summary")
for name, (mae, rmse, r2) in results.items():
    print(f"{name:<12} -> MAE: {mae:.4f} | RMSE: {rmse:.4f} | R²: {r2:.4f}")

best_by_rmse = min(results.items(), key=lambda kv: kv[1][1])[0]
print(f"\nBest model on TEST (by RMSE): {best_by_rmse}")
